{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GUSzTyJgUkB"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet -U torch torchmetrics lightning torchvision seaborn nibabel\n",
        "!git clone https://github.com/PCiunkiewicz/ensf-619.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd5umX13mr21"
      },
      "outputs": [],
      "source": [
        "# Optional for nightly release; may need to update cuda version tag\n",
        "!pip install -U --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_-CSaU47Td"
      },
      "outputs": [],
      "source": [
        "# Optional for TPU support; may need to update tpu-pytorch target url\n",
        "!pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGGPHaHxOwli"
      },
      "outputs": [],
      "source": [
        "!git -C ensf-619/ pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSmGn0VggnP0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, '/content/ensf-619/final_project')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from paths import MODEL_PATH\n",
        "from data import load_data, DANNDataset\n",
        "from dann import DeepCascadeDANN\n",
        "from transform import DeepCascadeTransform\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dT3iHuLcgz6F"
      },
      "outputs": [],
      "source": [
        "SIZE = 164\n",
        "\n",
        "target_images, _ = load_data(size=SIZE, mode='newborn')\n",
        "target_images = torch.tensor(target_images, dtype=torch.float32)\n",
        "target_images = target_images.unsqueeze(1)\n",
        "\n",
        "src_images, masks = load_data(size=SIZE, mode='negative')\n",
        "src_images = torch.tensor(src_images, dtype=torch.float32)\n",
        "src_images = src_images.unsqueeze(1)\n",
        "masks = torch.tensor(masks, dtype=torch.float32)\n",
        "\n",
        "transform = DeepCascadeTransform(size=SIZE)\n",
        "dataset = DANNDataset(src_images, masks, target_images, transform=transform)\n",
        "train_ds, val_ds = random_split(dataset, [0.8, 0.2])\n",
        "val_ds.dataset.val = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDesKNLhZRBw"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/ENSF619/models/DeepCascadeDANN/lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbqy5Ua8g7DH"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "model = DeepCascadeDANN(\n",
        "    # depth_str='ikikii',\n",
        "    depth_str='iiiii',\n",
        "    img_size=SIZE,\n",
        "    beta=0.01,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=200,\n",
        "    precision='16-mixed',\n",
        "    default_root_dir=MODEL_PATH / 'DeepCascadeDANN',\n",
        "    accelerator='auto',\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(save_weights_only=True, mode='min', monitor='val_loss'),\n",
        "        LearningRateMonitor('epoch')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# lr_finder = tuner.lr_find(model, train_dataloader, val_dataloader)\n",
        "# fig = lr_finder.plot(suggest=True)\n",
        "# fig.show()\n",
        "\n",
        "# compiled_model = torch.compile(model) # Complex64 not yet supported for compile.\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "model = DeepCascadeDANN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}